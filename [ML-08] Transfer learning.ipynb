{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lab30041954/ML_IESE_Course/blob/main/%5BML-08%5D%20Transfer%20learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0234edfe-9679-457c-9b48-9582f021cd96",
      "metadata": {
        "id": "0234edfe-9679-457c-9b48-9582f021cd96"
      },
      "source": [
        "# [ML-08] Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e12dc146-275f-4afd-ae0a-cefe91e69288",
      "metadata": {
        "id": "e12dc146-275f-4afd-ae0a-cefe91e69288"
      },
      "source": [
        "## What is transfer learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4a41065-b14c-42bb-a5cd-34008beb3ec3",
      "metadata": {
        "id": "a4a41065-b14c-42bb-a5cd-34008beb3ec3"
      },
      "source": [
        "**Transfer learning** is a technique used in machine learning to leverage the knowledge gained from one task to improve the performance on another related task. This typically involves taking a model that has been pre-trained on a big data set and adapting it for the new task, by updating the model's parameter values with new data, specific for the new task. The data set used in this updating step is typically much smaller than the pre-training data set, which allows us to save money and time.\n",
        "\n",
        "Transfer learning is critical in domains where data are only available in small amounts, or would be very expensive to collect. Since big data sets are needed to scape overfitting for complex models, it is currently common practice in many business applications. In both **computer vision** and in **natural language processing**, we can profit from existing models that have been released by their developers.\n",
        "\n",
        "Transfer learning has two components:\n",
        "\n",
        "* The pre-trained model. These models are usually extracted from public repositories, called **hubs**.\n",
        "\n",
        "* The new data. These data have to be specific of the new task, called the **downstream task**.\n",
        "\n",
        "When can transfer learning help you? When you can find a pre-trained model with the same kind of inputs, and there is a commonality in the tasks of the two models. It makes a difference because, when training a neural network model from scratch, we start with random parameter values. These initial values don't make any sense for the task on which we are training the model. Starting with the parameter values learned in a previous training, we are much closer to the optimal values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd9dc957-8d85-46d2-b3c4-4aefa5dbf967",
      "metadata": {
        "id": "cd9dc957-8d85-46d2-b3c4-4aefa5dbf967"
      },
      "source": [
        "## Sources of pre-trained models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed88ab8d-3834-4fc0-9b3e-1182393926e4",
      "metadata": {
        "id": "ed88ab8d-3834-4fc0-9b3e-1182393926e4"
      },
      "source": [
        "* **Keras**. The Keras team has created two hubs, **KerasCV** (computer vision) and **KerasNLP** (natural language processing), which can be accessed by means of specific Python packages. They have selected the models, so there is plenty of choice and you will not miss anything relevant. These are (relatively) small models, and most of them can be managed by your computer.\n",
        "\n",
        "* **Hugging Face**. The favorite hub, so far independent of the big corporations. In addition to the \"serious\" models that you can find in the Keras hubs, you will find in Hugging Face thousands of models, uploaded by the (registered) users, which are just retrained versions of the those available in the Keras hubs. When this is being written, Hugging face website claims to provide 1,990,650 models.\n",
        "\n",
        "* **ModelScope** is a copy cat of Hugging Face, launched by Alibaba Cloud. It is much smaller, although it is growing fast. It currently provides 94,634 models.\n",
        "\n",
        "* **Kaggle Models**. Kaggle started as an independent platform for data science and machine learning competitions, adding later a hub for data sets. Everybody could post data, notebooks, etc. It was purchased by Google in 2017. Right now, those competitions have lost their glamour, but Kaggle offers, besides the data sets, a mix of courses, notebooks and pre-trained models. Though the (registered) members of the Kaggle community can post their models, as in Hugging Face, the relevant stuff can be easily found.\n",
        "\n",
        "* **TensorFlow Hub**. It was initially part of the Keras/TensorFlow combo, but was integrated with Kaggle Models in November 2022.\n",
        "\n",
        "* **Ollama**. An open-source project that serves as a platform for running LLMs on your local machine. Not exactly user-friendly, as many open-source projects, but quite powerful. It is presented as if you had to manage it from the shell, but there is a Python package that provides an easy way to integrate it in your workflow.\n",
        "\n",
        "Some of this will show up in this course, in this and the following two lectures."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3e6b86f-7bc5-4782-a16c-953300e14332",
      "metadata": {
        "id": "f3e6b86f-7bc5-4782-a16c-953300e14332"
      },
      "source": [
        "## Transfer learning for CNN models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb96441-04ff-4ac2-9a37-2faefc6428cc",
      "metadata": {
        "id": "abb96441-04ff-4ac2-9a37-2faefc6428cc"
      },
      "source": [
        "Keras provides some powerful image classifiers, pre-trained on an ML classic, the **ImageNet** data set. ImageNet is the outcome of a project started by FF Lei, then a professor at Princeton, in 2006. We use one of these models, **VGG16**, in this lecture. It is based on a CNN architecture which is similar to the one used in the preceding lecture, though a bit bigger. Even if VGG16 is a dwarf (below 20M parameters) compared to the top popular large language models, it will suffice for understanding the dynamics of transfer learning.\n",
        "\n",
        "To illustrate this, let us take the model summarized below, trained on the MNIST data in the preceding lecture.\n",
        "\n",
        "```\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "│ input_layer_1 (InputLayer)      │ (None, 28, 28, 1)      │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_1 (Conv2D)               │ (None, 26, 26, 32)     │           320 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_1 (MaxPooling2D)  │ (None, 13, 13, 32)     │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_2 (Conv2D)               │ (None, 11, 11, 64)     │        18,496 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_2 (MaxPooling2D)  │ (None, 5, 5, 64)       │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_3 (Conv2D)               │ (None, 3, 3, 64)       │        36,928 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ flatten_1 (Flatten)             │ (None, 576)            │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_1 (Dense)                 │ (None, 64)             │        36,928 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_2 (Dense)                 │ (None, 10)             │           650 │\n",
        "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        "```\n",
        "\n",
        "In the context of this lecture, who would say that it is the pre-trained model. We can see it as composed of two parts:\n",
        "\n",
        "* The **convolutional base**, a stack of three `Conv2D` layers and two `MaxPooling2D` layers. This part encodes the picture as a vector of length 576. This transformation is what we called an **embedding** (see next lecture). Because of the pretraining, the embedding generates features that are appropriate for recognizing shapes and corners.\n",
        "\n",
        "* The **top classifier**, which is like an MLP model with a hidden layer of 64 nodes whose input is a vector of length 576. The `Flatten` layer does not have any parameter, so it can be included in this part or in the base. This part classifies the embedding vectors as digits.\n",
        "\n",
        "Now, suppose that we switch from digit recognition to letter recognition. Our model can be based on the same network architecture, except for the last `Dense` layer, which will be adapted so that it will ouptput 26 class probabilities (English alphabet) instead of 10. If we us agree that the convolutional base, as it is, is also good for the new task, we can freeze the parameter values of that part, and train the classifier. In practice, it would be as if we were using the embedding vectors as the features for a classification model. So, instead of transfer learning, we could call this **feature engineering**.\n",
        "\n",
        "We can also unfreeze part of the convolutional base (*e.g*. the last `Conv2D` layer), or the whole thing, modifying so the parameter values. This is called  **fine-tuning**, because, even if the parameter values change, they don't change much. We will see in the following example how easy are these tricks in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399cb5cc-ef2b-4cc4-b7bf-033dba061586",
      "metadata": {
        "id": "399cb5cc-ef2b-4cc4-b7bf-033dba061586"
      },
      "source": [
        "## Example - The Dogs vs Cats data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05872334-7099-46da-a9c0-4f7ca2571922",
      "metadata": {
        "id": "05872334-7099-46da-a9c0-4f7ca2571922"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c07308a-ca30-476e-b1da-087881e90e0e",
      "metadata": {
        "id": "1c07308a-ca30-476e-b1da-087881e90e0e"
      },
      "source": [
        "Web services are often protected with a challenge that is supposed to be easy for people, but difficult for computers. This is often called **CAPTCHA** (Completely Automated Public Turing test to tell Computers and Humans Apart) or **HIP** (Human Interactive Proof). CAPTCHA's are used for many purposes, such as to reduce email and blog spam, preventing brute force attacks on web site passwords.\n",
        "\n",
        "**ASIRRA** (Animal Species Image Recognition for Restricting Access) is a CAPTCHA that works by asking users to distinguish between photographs of **cats** and **dogs**. This task is difficult for computers, but studies have shown that people can accomplish it quickly and accurately. ASIRRA is unique because of its partnership with **Petfinder.com**, the world's largest site devoted to finding homes for homeless pets. They have provided Microsoft Research with over three million images of cats and dogs, manually classified by people at thousands of animal shelters across the United States.\n",
        "\n",
        "This example uses part of this data set, released for the *Dogs vs. Cats* Kaggle competition. It is inspired by the approach to transfer learning taken in chapter 9 of the Keras book (Chollet, 2021)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa137995-33e3-4a7d-a0d8-045b312aa83e",
      "metadata": {
        "id": "aa137995-33e3-4a7d-a0d8-045b312aa83e"
      },
      "source": [
        "### The data set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fedb7770-3494-4315-8452-7230495641d7",
      "metadata": {
        "id": "fedb7770-3494-4315-8452-7230495641d7"
      },
      "source": [
        "The *Dogs vs. Cats* data set is available from many sources, including Kaggle and Keras. It contains 25,000 images of dogs and cats (12,500 from each class). The pictures are medium-resolution color JPG files. In this example, we use 2,000 images for training and 1,000 for testing. Both data sets are balanced.\n",
        "\n",
        "The images have been pre-processed so they all have resolution 150 $\\times$ 150. Setting a fixed resolution is needed for training and later application of the model obtained. This resolution, which has no special virtue, has been chosen so that this notebook can be run in either your laptop, or in Google Colab, without pain. Note that you don't need to use square images, but it looks like a reasonable choice, since in the original image set we found both landscape and portrait orientation. The conversion is an easy job which can be done with the Python package `opencv`, which appears below.\n",
        "\n",
        "The data come as a four (zipped) folders of JPG files, `dogs-train`, `dogs-test`, `cats-train` and `cats-test`. As we have mentioned in the preceding lecture, an image is just an array with two spatial axes, called **height** and **width**, and a **depth** axis. For an RGB image, the dimension of the depth axis would be 3, since the image has three color channels, red, green, and blue. The height and width depend on the resolution of the images, which is not fixed here. So, before being converted to NumPy arrays that can be inputted to a neural network, all the images must be resized to a common resolution.\n",
        "\n",
        "Sources:\n",
        "\n",
        "1. W Cukierski (2013) *Dogs vs. Cats*, `https://kaggle.com/competitions/dogs-vs-cats`.\n",
        "\n",
        "2. F Chollet (2021), *Deep Learning with Python*, Manning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c796c32-b475-4b82-82ca-dd66c087ae0f",
      "metadata": {
        "id": "8c796c32-b475-4b82-82ca-dd66c087ae0f"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8a9045-bb7e-4070-b86d-73e252fbfc58",
      "metadata": {
        "id": "de8a9045-bb7e-4070-b86d-73e252fbfc58"
      },
      "source": [
        "Q1. Create a folder in the working directory of your current Python kernel and download there the ZIP files. Unzip these files, so you get the four folders mentioned above. The folders with training data contain contain 1,000 JPG files each, while those with test data contain 500 files each.\n",
        "\n",
        "Q2. Write a function which reads the JPG files as NumPy arrays (the `matplot.pyplot` function `imread()` can be used here), and reshapes the arrays to shape `(1, 150, 150, 3)`. Apply your function to all the JPG files and concatenate the resulting arrays in such a way that you obtain an array `X_train`, with shape `(2000, 150, 150, 3)` and an array `X_test`, with shape `(1000, 150, 150, 3)`. Create the corresponding target vectors, with value 1 for the dogs and value 0 for the cats.\n",
        "\n",
        "Q3. Train a CNN model from scratch and evaluate it. You can build the network by stacking four pairs of `Conv2D` and `MaxPooling2D` layers, a `Flatten` layer and two `Dense` layers. The last layer must output the two class probabilities.\n",
        "\n",
        "Q4. Import the pre-trained model VGG16 from the Keras module `applications`. Include the convolutional base but not the top classifier. Then, freeze the parameter values of the convolutional base and add a densely connected classifier on top.\n",
        "\n",
        "Q5. Train the new model and compare its performance on the test data with that of the model of question Q3."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbd56717-7058-44de-bb4f-75c8cb512f92",
      "metadata": {
        "id": "bbd56717-7058-44de-bb4f-75c8cb512f92"
      },
      "source": [
        "### Q1a. Creating a data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95dbe748-53a3-412c-adb4-779f79d929f8",
      "metadata": {
        "id": "95dbe748-53a3-412c-adb4-779f79d929f8"
      },
      "source": [
        "The package `os` (included in the Python Standard Library) contains a collection of functions for common **operating system commands**. You can create, delete and copy files and folders from the Python kernel. We import it as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "878c44eb-8da0-4bf0-99fc-e198a50bdbae",
      "metadata": {
        "id": "878c44eb-8da0-4bf0-99fc-e198a50bdbae"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f3a026-d8e7-44ec-8fcf-129d2883b38e",
      "metadata": {
        "id": "82f3a026-d8e7-44ec-8fcf-129d2883b38e"
      },
      "source": [
        "The function `mkdir()` creates a folder, that will be appear in the working directory of the current kernel, unless you specify a specific path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "9ebb6ac1-244f-4010-8d9b-6bc94ea461e8",
      "metadata": {
        "id": "9ebb6ac1-244f-4010-8d9b-6bc94ea461e8"
      },
      "outputs": [],
      "source": [
        "os.mkdir('data/')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ce050b3-a028-4b79-94fb-9006d70a667c",
      "metadata": {
        "id": "4ce050b3-a028-4b79-94fb-9006d70a667c"
      },
      "source": [
        "*Note*. The slash (`/`) at the end of the folder name is not needed, but it may help to distinguish between files and folders."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d4ef6e6-8923-4775-ae7f-e2d4a29681c5",
      "metadata": {
        "id": "5d4ef6e6-8923-4775-ae7f-e2d4a29681c5"
      },
      "source": [
        "### Q1b. Dowloading the zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e77780-e673-47dd-b21e-0e4499fc6c4e",
      "metadata": {
        "id": "b9e77780-e673-47dd-b21e-0e4499fc6c4e"
      },
      "source": [
        "The package `requests`, included in the Anaconda distribution, and also available in Colab notebooks, provides **HTTP functionality**. We import it as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8d710f5d-fe82-4e52-acf1-1ba45e92701f",
      "metadata": {
        "id": "8d710f5d-fe82-4e52-acf1-1ba45e92701f"
      },
      "outputs": [],
      "source": [
        "import requests"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "780b80d7-42f4-46bf-9851-9574c060e1dd",
      "metadata": {
        "id": "780b80d7-42f4-46bf-9851-9574c060e1dd"
      },
      "source": [
        "We specify our GitHub pathas usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "211c0da0-a9de-4429-8d64-a6e4c6225f13",
      "metadata": {
        "id": "211c0da0-a9de-4429-8d64-a6e4c6225f13"
      },
      "outputs": [],
      "source": [
        "gitpath = 'https://raw.githubusercontent.com/lab30041954/Data/main/'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b801f9-e148-4c2a-8a9e-ced2ee133eb0",
      "metadata": {
        "id": "a7b801f9-e148-4c2a-8a9e-ced2ee133eb0"
      },
      "source": [
        "To select the files to be downloaded, we will loop over ther following list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f9cabccb-ed9e-423f-9b11-db62e566f93a",
      "metadata": {
        "id": "f9cabccb-ed9e-423f-9b11-db62e566f93a"
      },
      "outputs": [],
      "source": [
        "gitlist = ['cats_train.zip', 'cats_test.zip', 'dogs_train.zip', 'dogs_test.zip']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6920d093-7428-4e9b-8495-3e7a2ea93f7c",
      "metadata": {
        "id": "6920d093-7428-4e9b-8495-3e7a2ea93f7c"
      },
      "source": [
        "The resources involved in the loop are:\n",
        "\n",
        "* The `requests` function `get()` will send a **GET request** to GitHub. If the response is positive, the files specified will be read by the Python kernel. The argument `stream=True` is used for efficiency, but it probably does not make a difference here.\n",
        "\n",
        "* The Python function `open()` will create a **file object** (whose name plays no role). Since these files don't exist exist, new (empty) files will be created. Then, the method `.write()` will write the content of the ZIP files to these new files. The argument `mode='wb'` means *write in binary mode*.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c4511c1b-509e-47cd-8064-2fc9978f2820",
      "metadata": {
        "id": "c4511c1b-509e-47cd-8064-2fc9978f2820"
      },
      "outputs": [],
      "source": [
        "for f in gitlist:\n",
        "\tr = requests.get(gitpath + f, stream=True)\n",
        "\tconn = open('data/' + f, mode='wb')\n",
        "\tconn.write(r.content)\n",
        "\tconn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd354ac1-2186-40a4-9f75-9b6fde2595f9",
      "metadata": {
        "id": "cd354ac1-2186-40a4-9f75-9b6fde2595f9"
      },
      "source": [
        "## Q1c. Unzipping and removing the zip files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The package `zipfile` (also in the Python Standard Library) provides resources for zipping and unizipping files and folders in a simple way. We import it as:"
      ],
      "metadata": {
        "id": "p_MPvqkXf-o1"
      },
      "id": "p_MPvqkXf-o1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "d98e75dd-29a0-4105-8d21-c838c257a2eb",
      "metadata": {
        "id": "d98e75dd-29a0-4105-8d21-c838c257a2eb"
      },
      "outputs": [],
      "source": [
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To unizp the four files in a row, we create first a list of the files to unzip. Note that the `os` function `listdir()` lists both files and folders, so we have to exclude the folders from the list."
      ],
      "metadata": {
        "id": "TRvxnGlhgBwB"
      },
      "id": "TRvxnGlhgBwB"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "2ccfb8c5-d919-42bb-9971-e1435568b8ae",
      "metadata": {
        "id": "2ccfb8c5-d919-42bb-9971-e1435568b8ae"
      },
      "outputs": [],
      "source": [
        "ziplist = [f for f in os.listdir('data/') if 'zip' in f]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we loop over this list unzipping and removing the ZIP files one by one. The resources involved are:\n",
        "\n",
        "* The `zipfile` function `Zipfile()` creates a **ZipFile object** associated to the specified file.\n",
        "\n",
        "* The method `extractall()` extracts the content of the ZIP file and writes it to disk.\n",
        "\n",
        "* The Python keyword `del` is used to delete objects (from the Python kernel, not from the disk).\n",
        "\n",
        "* The `os` function `remove()` is used to remove files from disk."
      ],
      "metadata": {
        "id": "JA9I1HvvgM0R"
      },
      "id": "JA9I1HvvgM0R"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9c85c1ae-5dba-4a7d-b4d5-a05327dd526a",
      "metadata": {
        "id": "9c85c1ae-5dba-4a7d-b4d5-a05327dd526a"
      },
      "outputs": [],
      "source": [
        "for f in ziplist:\n",
        "\tzf = zipfile.ZipFile('data/' + f, 'r')\n",
        "\tzf.extractall('data/')\n",
        "\tdel zf\n",
        "\tos.remove('data/' + f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us check that the process was carried out as expected. First, the folder `data` contains four folders, with the appropriate names."
      ],
      "metadata": {
        "id": "Fo2AaBNeg-r3"
      },
      "id": "Fo2AaBNeg-r3"
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('data/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubNionLhhJyp",
        "outputId": "8c398c08-f18e-443b-826a-1adf0148f90c"
      },
      "id": "ubNionLhhJyp",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['dogs_test', 'dogs_train', 'cats_test', 'cats_train']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, every folder contains the expected number of files. For instance:"
      ],
      "metadata": {
        "id": "YX0khtcuhMcf"
      },
      "id": "YX0khtcuhMcf"
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir('data/dogs_train/'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcIszzHUhUPW",
        "outputId": "5c949fd8-bca0-4148-ab89-83aaca06ef28"
      },
      "id": "LcIszzHUhUPW",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef37c44e-029f-40d0-b0bc-5212c2e09227",
      "metadata": {
        "id": "ef37c44e-029f-40d0-b0bc-5212c2e09227"
      },
      "source": [
        "### Q2a. Converting images to tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the resources to be used for the conversion."
      ],
      "metadata": {
        "id": "Gruhq-8EhrWr"
      },
      "id": "Gruhq-8EhrWr"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "21cabbfb-fe8c-4000-b164-d8aeabdc0d37",
      "metadata": {
        "id": "21cabbfb-fe8c-4000-b164-d8aeabdc0d37"
      },
      "outputs": [],
      "source": [
        "import numpy as np, matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our training job, we create a NumPy array from every JPG. These arrays must have the same shape so they can be packed in the training and test features arrays, and processed by a neural network model. We write a function to loop over the folders just created in question Q1. The resources involved are:\n",
        "\n",
        "* The function `imread()` converts a JPG file to a NumPy array. This is a classic Matplotlib function incorporated by many packages. It works the same for other image formats (such as BMP or PNG).\n",
        "\n",
        "* Every picture will be converted to a 3D array of shape `(150, 150, 3)`. This will be reshaped to `(1, 150, 150, 3)`. Remember that, in the preceding lecture, the input of the convolutional network was a 4D array."
      ],
      "metadata": {
        "id": "ukRgHrfnh8Xn"
      },
      "id": "ukRgHrfnh8Xn"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "05b6e9c1-6d70-4f97-8807-60a6f20dc4fc",
      "metadata": {
        "id": "05b6e9c1-6d70-4f97-8807-60a6f20dc4fc"
      },
      "outputs": [],
      "source": [
        "def img_to_arr(f):\n",
        "    arr = plt.imread(f)\n",
        "    reshaped_arr = arr.reshape(1, 150, 150, 3)\n",
        "    return reshaped_arr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfebe26-8e45-4952-9244-8764b5c45e0f",
      "metadata": {
        "id": "ddfebe26-8e45-4952-9244-8764b5c45e0f"
      },
      "source": [
        "## Q2b. Training data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data will be made of two arrays: (a) the features, packed in an array `X_train` of shape `(2000, 150, 150, 3)`, and (b) the target `y_train`, which will be a 1D array with 1's (dogs) and 0's (cats). We create `X_train` using the first dog picture, so it has shape `(1, 150, 150, 3)`."
      ],
      "metadata": {
        "id": "88Srf2oxjL2G"
      },
      "id": "88Srf2oxjL2G"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7bdf6b00-3e09-4a01-980c-8fbd22dabea0",
      "metadata": {
        "id": "7bdf6b00-3e09-4a01-980c-8fbd22dabea0",
        "outputId": "15efeed6-3c06-4095-f940-824c0d273beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 150, 150, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "X_train = img_to_arr('data/dogs_train/' + os.listdir('data/dogs_train')[0])\n",
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we loop over the folder `dogs-train', adding dogs one by one with NumPy function `concatenate()`. By default, the concatenation is carried out along the first axis (`axis=0`)."
      ],
      "metadata": {
        "id": "ulOWD13vjj1I"
      },
      "id": "ulOWD13vjj1I"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "4708de14-d854-48c1-91c8-c6267a355735",
      "metadata": {
        "id": "4708de14-d854-48c1-91c8-c6267a355735"
      },
      "outputs": [],
      "source": [
        "for i in range(1, 1000):\n",
        "    X_train = np.concatenate([X_train, img_to_arr('data/dogs_train/' + os.listdir('data/dogs_train')[i])])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the cats from the training set."
      ],
      "metadata": {
        "id": "fZN9zqjkjqlo"
      },
      "id": "fZN9zqjkjqlo"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "5f7a4c80-c683-4907-81f6-e1fb574039a3",
      "metadata": {
        "id": "5f7a4c80-c683-4907-81f6-e1fb574039a3"
      },
      "outputs": [],
      "source": [
        "for i in range(1000):\n",
        "    X_train = np.concatenate([X_train, img_to_arr('data/cats_train/' + os.listdir('data/cats_train')[i])])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we rescale the pixel intensities to the 0-1 range."
      ],
      "metadata": {
        "id": "a1OCc2ILjyzT"
      },
      "id": "a1OCc2ILjyzT"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b8703658-cbaa-4542-8986-176918a2a8d1",
      "metadata": {
        "id": "b8703658-cbaa-4542-8986-176918a2a8d1"
      },
      "outputs": [],
      "source": [
        "X_train = X_train/255"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The NumPy functions `ones()` and `zeros()` allow for the creation of arrays of the specified shape, filled with 1's and 0's, respectively. We out first the 1's, so they are the target values for the dog pictures."
      ],
      "metadata": {
        "id": "RE6iVKnqj4Vm"
      },
      "id": "RE6iVKnqj4Vm"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "bd632f3f-a345-4a55-8cfa-45e8798062a5",
      "metadata": {
        "id": "bd632f3f-a345-4a55-8cfa-45e8798062a5"
      },
      "outputs": [],
      "source": [
        "y_train = np.concatenate([np.ones(1000), np.zeros(1000)])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We check now that the shapes of these arrays are the expected ones."
      ],
      "metadata": {
        "id": "Aw6UH4cFkAHF"
      },
      "id": "Aw6UH4cFkAHF"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f53b500f-950c-47b3-9a00-44b512ab2ad7",
      "metadata": {
        "id": "f53b500f-950c-47b3-9a00-44b512ab2ad7",
        "outputId": "98af774c-e225-42fc-ce8f-89c33cf7b544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 150, 150, 3), (2000,))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d95599a-8f04-43ca-bea6-ee2e9520266c",
      "metadata": {
        "id": "4d95599a-8f04-43ca-bea6-ee2e9520266c"
      },
      "source": [
        "### Q2c. Test data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We follow the same steps for the test data."
      ],
      "metadata": {
        "id": "GOmWzBJykDbr"
      },
      "id": "GOmWzBJykDbr"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2adc2de3-535e-418a-85a9-1b94be0008aa",
      "metadata": {
        "id": "2adc2de3-535e-418a-85a9-1b94be0008aa",
        "outputId": "e6b3f880-cea0-4a4f-f98d-a7c26b50849a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 150, 150, 3), (1000,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X_test = img_to_arr('data/dogs_test/' + os.listdir('data/dogs_test')[0])\n",
        "for i in range(1, 500):\n",
        "    X_test = np.concatenate([X_test, img_to_arr('data/dogs_test/' + os.listdir('data/dogs_test')[i])])\n",
        "for i in range(500):\n",
        "    X_test = np.concatenate([X_test, img_to_arr('data/cats_test/' + os.listdir('data/cats_test')[i])])\n",
        "X_test = X_test/255\n",
        "y_test = np.concatenate([np.ones(500), np.zeros(500)])\n",
        "X_test.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33f34cb-5617-4133-acd0-3387a14f3835",
      "metadata": {
        "id": "c33f34cb-5617-4133-acd0-3387a14f3835"
      },
      "source": [
        "### Q3. Training a CNN model from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the Keras function `Input()` and the modules `models` and `layers`, as in the previous lectures.\n"
      ],
      "metadata": {
        "id": "XvLQEbndc4qu"
      },
      "id": "XvLQEbndc4qu"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "00a00837-9b98-4771-b387-950fdb43446c",
      "metadata": {
        "id": "00a00837-9b98-4771-b387-950fdb43446c"
      },
      "outputs": [],
      "source": [
        "from keras import Input, models, layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we specify the shape of the input tensor, which corresponds to an RGB image with resolution 150 $\\times$ 150."
      ],
      "metadata": {
        "id": "L6qvS3H8dAqq"
      },
      "id": "L6qvS3H8dAqq"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5b402a20-5acf-4675-b0c5-3058fe8bc64c",
      "metadata": {
        "id": "5b402a20-5acf-4675-b0c5-3058fe8bc64c"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(150, 150, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the hidden layers. As in the preceding lecture we stack convolutional blocks (`Conv2D` plus `MaxPooling2D`). Since we are dealing with bigger images, we make the network larger, including a fourth block. The depth of the feature maps progressively increases in the network (from 32 to 128), while the size decreases (from 150 $\\times$ 150 to 7 $\\times$ 7). As we will see in the summary below, flattening the output of the fourth convolutional block leaves us with a tensor of length 6,272, so we reduce the dimensionality with a final `Dense` layer. This last layer returns a vector of length 512 which is expected to provide a representation of the image that helps the dog/cat classification. This dimensionality reduction is a standard procedure."
      ],
      "metadata": {
        "id": "Znx8nrf7dFl6"
      },
      "id": "Znx8nrf7dFl6"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "2e4e956f-5311-4f72-8ac5-e2c6a53d33d2",
      "metadata": {
        "id": "2e4e956f-5311-4f72-8ac5-e2c6a53d33d2"
      },
      "outputs": [],
      "source": [
        "x1 = layers.Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x2 = layers.MaxPooling2D((2, 2))(x1)\n",
        "x3 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "x4 = layers.MaxPooling2D((2, 2))(x3)\n",
        "x5 = layers.Conv2D(128, (3, 3), activation='relu')(x4)\n",
        "x6 = layers.MaxPooling2D((2, 2))(x5)\n",
        "x7 = layers.Conv2D(128, (3, 3), activation='relu')(x6)\n",
        "x8 = layers.MaxPooling2D((2, 2))(x7)\n",
        "x9 = layers.Flatten()(x8)\n",
        "x10 = layers.Dense(512, activation='relu')(x9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the output layer, which returns the predicted class probabilities."
      ],
      "metadata": {
        "id": "AujyT7o-dU21"
      },
      "id": "AujyT7o-dU21"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f1f0f3e0-2d5c-49e2-be1a-ead385c3be32",
      "metadata": {
        "id": "f1f0f3e0-2d5c-49e2-be1a-ead385c3be32"
      },
      "outputs": [],
      "source": [
        "output_tensor = layers.Dense(2, activation='softmax')(x10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The successive application of these functions make the CNN model, which works as a flow that starts with the input tensor and ends with the output tensor."
      ],
      "metadata": {
        "id": "0ujAeXTadYrz"
      },
      "id": "0ujAeXTadYrz"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d8c1e9c8-5329-492a-8ccf-8ed946b6e4e6",
      "metadata": {
        "id": "d8c1e9c8-5329-492a-8ccf-8ed946b6e4e6"
      },
      "outputs": [],
      "source": [
        "clf1 = models.Model(input_tensor, output_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The table returned by the method `.summary()` illustrates this network architecture, with involves 3.45M parameters."
      ],
      "metadata": {
        "id": "rZGKd3AFdc1s"
      },
      "id": "rZGKd3AFdc1s"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f340844c-5e7b-4a4c-aa91-8de87ed40621",
      "metadata": {
        "id": "f340844c-5e7b-4a4c-aa91-8de87ed40621",
        "outputId": "b2fb0b15-b362-4421-c868-25cfcccfcdab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6272\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m3,211,776\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │         \u001b[38;5;34m1,026\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6272</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,776</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,026</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,453,634\u001b[0m (13.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,453,634</span> (13.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,453,634\u001b[0m (13.17 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,453,634</span> (13.17 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "clf1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we compile the model. Nothing new here."
      ],
      "metadata": {
        "id": "cWKk9BzedhuW"
      },
      "id": "cWKk9BzedhuW"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "d00da26b-2345-49cd-a5ba-1e683b816be5",
      "metadata": {
        "id": "d00da26b-2345-49cd-a5ba-1e683b816be5"
      },
      "outputs": [],
      "source": [
        "clf1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the method `.fit()`, we train and test the model with the data sets that were built above. 10 epochs are enough to see the limitations of this approach. We get about 71.8% accuracy on the test data (not negligeable), but with a clear overfitting issue. The training data do not seem to be enough for so many parameters."
      ],
      "metadata": {
        "id": "qwsIBh8edn0H"
      },
      "id": "qwsIBh8edn0H"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "15801c2a-502f-4a23-acc9-45e04f3491bb",
      "metadata": {
        "id": "15801c2a-502f-4a23-acc9-45e04f3491bb",
        "outputId": "1a52014b-81ea-4ac5-fd3e-70006aace7f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 112ms/step - acc: 0.5064 - loss: 0.7401 - val_acc: 0.5000 - val_loss: 0.6919\n",
            "Epoch 2/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - acc: 0.5343 - loss: 0.6898 - val_acc: 0.5590 - val_loss: 0.6563\n",
            "Epoch 3/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - acc: 0.6011 - loss: 0.6581 - val_acc: 0.5680 - val_loss: 0.6743\n",
            "Epoch 4/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - acc: 0.6060 - loss: 0.6497 - val_acc: 0.6340 - val_loss: 0.6206\n",
            "Epoch 5/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - acc: 0.6423 - loss: 0.6196 - val_acc: 0.5690 - val_loss: 0.6901\n",
            "Epoch 6/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - acc: 0.6559 - loss: 0.6119 - val_acc: 0.6510 - val_loss: 0.6126\n",
            "Epoch 7/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - acc: 0.6996 - loss: 0.5607 - val_acc: 0.6870 - val_loss: 0.5757\n",
            "Epoch 8/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 30ms/step - acc: 0.7447 - loss: 0.4933 - val_acc: 0.7070 - val_loss: 0.5915\n",
            "Epoch 9/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - acc: 0.7868 - loss: 0.4347 - val_acc: 0.6530 - val_loss: 0.6471\n",
            "Epoch 10/10\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - acc: 0.8115 - loss: 0.3909 - val_acc: 0.7180 - val_loss: 0.6461\n"
          ]
        }
      ],
      "source": [
        "clf1.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46c27b91-9c17-4ebd-8de2-c86329b00886",
      "metadata": {
        "id": "46c27b91-9c17-4ebd-8de2-c86329b00886"
      },
      "source": [
        "### Q4a. Pre-trained CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras module `applications` is a legacy of Keras 2 that provides a limited supply or pre-trained models (compared to the current repositories), but is more than enough for this example. It contains a collection of deep learning models that are made available alongside pre-trained weights. These models can be used for prediction, feature extraction, and fine-tuning. The model VGG16 is a (relatively) simple CNN model with a convolutional base made of `Conv2D` and `MaxPooling2D` layers. Importing this model is straightforward."
      ],
      "metadata": {
        "id": "ohQBAU5ZdwTe"
      },
      "id": "ohQBAU5ZdwTe"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "53842669-08e5-4d6c-9818-4da96a3d2dc3",
      "metadata": {
        "id": "53842669-08e5-4d6c-9818-4da96a3d2dc3"
      },
      "outputs": [],
      "source": [
        "from keras.applications import VGG16"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate a VGG16 model. Note the choices made:\n",
        "\n",
        "* The argument `weights='imagenet'` specifies that the initial parameter values are those obtained from training the model with the ImageNet data. We can update them in the training process, or freeze them, which is what we do in this example. We can also update only a subset of the weights (typically those from the last layers), as proposed later .\n",
        "\n",
        "* The model can be seen as a convolutional base plus a densely connected classifier on top. With the argument `include_top=False`, this classifier, which would return probabilities for the 1,000 ImageNet classes, is discarded.\n",
        "\n",
        "* The argument `input_shape=(150, 150, 3)` is needed only for the summary below. When creating our new model, the input shape will be specified in the usual way.\n",
        "\n",
        "The summary shows that the VGG16 base is made of five convolutional blocks. These blocks contain two or three `Conv2D` layers. The height and width are kept constant with a trick called **padding** (look at the Keras book is you are interested)."
      ],
      "metadata": {
        "id": "pfwhgIvfeAbm"
      },
      "id": "pfwhgIvfeAbm"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b1f796ff-79a0-43ff-b07a-0643399c10d6",
      "metadata": {
        "id": "b1f796ff-79a0-43ff-b07a-0643399c10d6",
        "outputId": "61a4edb3-c0ae-4535-f727-7cfe97a562a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 795
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"vgg16\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vgg16\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m75\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m37\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m9\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">75</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">9</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we freeze the parameter values of the convolutional base, so they will not be adapted to the cats vs dogs data. This is optional, and it is even possible to freeze only the initial layers, as suggested in the homework. Freezing the whole convolutional base is pretty easy:"
      ],
      "metadata": {
        "id": "nlqttjS0eXcv"
      },
      "id": "nlqttjS0eXcv"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "022dc994-2c35-45bd-859f-d87bd02aec4a",
      "metadata": {
        "id": "022dc994-2c35-45bd-859f-d87bd02aec4a"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c76c76e-f7e2-40d5-8e36-fc06f0de6e30",
      "metadata": {
        "id": "3c76c76e-f7e2-40d5-8e36-fc06f0de6e30"
      },
      "source": [
        "### Q4b. Adding a densely connected classifier on top"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we build a new network adding three layers on top of the convolutional base. The convolutional base is managed here as a single component. The top layers are the same as in the network of question Q3. Note that the 14,714,688 parameters of the convolutional base appear here as non-trainable."
      ],
      "metadata": {
        "id": "fXecSw_CekZK"
      },
      "id": "fXecSw_CekZK"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "f82c86bc-b065-4931-ad68-584807e92c2e",
      "metadata": {
        "id": "f82c86bc-b065-4931-ad68-584807e92c2e",
        "outputId": "600e3ba0-5f9d-44d0-c20e-cc90c23cc234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m150\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m2,097,408\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">150</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,812,610\u001b[0m (64.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,812,610</span> (64.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,097,922\u001b[0m (8.00 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,922</span> (8.00 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "input_tensor = Input(shape=(150, 150, 3))\n",
        "x1 = conv_base(input_tensor)\n",
        "x2 = layers.Flatten()(x1)\n",
        "x3 = layers.Dense(256, activation='relu')(x2)\n",
        "output_tensor = layers.Dense(2, activation='softmax')(x3)\n",
        "clf2 = models.Model(input_tensor, output_tensor)\n",
        "clf2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad1564e0-3f55-41dc-b899-fa1614951bc4",
      "metadata": {
        "id": "ad1564e0-3f55-41dc-b899-fa1614951bc4"
      },
      "source": [
        "### Q5. Training the new model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we compile and fit the new model. The improvement, compared to the smaller network of question Q3, is quite clear."
      ],
      "metadata": {
        "id": "8Zwv3tkhesVP"
      },
      "id": "8Zwv3tkhesVP"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "75632ec9-ea4c-4862-bfde-921e07546f63",
      "metadata": {
        "id": "75632ec9-ea4c-4862-bfde-921e07546f63",
        "outputId": "9095b0de-3fda-43ec-a445-b15b18cc2951",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 265ms/step - acc: 0.6115 - loss: 1.4214 - val_acc: 0.8600 - val_loss: 0.3088\n",
            "Epoch 2/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - acc: 0.9007 - loss: 0.2574 - val_acc: 0.8830 - val_loss: 0.2683\n",
            "Epoch 3/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 145ms/step - acc: 0.9466 - loss: 0.1596 - val_acc: 0.8840 - val_loss: 0.2553\n",
            "Epoch 4/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 146ms/step - acc: 0.9572 - loss: 0.1274 - val_acc: 0.8940 - val_loss: 0.2530\n",
            "Epoch 5/5\n",
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 147ms/step - acc: 0.9705 - loss: 0.1025 - val_acc: 0.8670 - val_loss: 0.3242\n"
          ]
        }
      ],
      "source": [
        "clf2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "clf2.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e82c67-7bc5-4d7e-b8ff-b05bf7ad9007",
      "metadata": {
        "id": "22e82c67-7bc5-4d7e-b8ff-b05bf7ad9007"
      },
      "source": [
        "### Removing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, you may remove the data from your computer. Note that in Google Colab, the data are deleted automatically unless you save them in your google Drive."
      ],
      "metadata": {
        "id": "UCEQBA3ce9ME"
      },
      "id": "UCEQBA3ce9ME"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "117eea0e-7386-41d9-a2ab-c31ebacb0b20",
      "metadata": {
        "id": "117eea0e-7386-41d9-a2ab-c31ebacb0b20"
      },
      "outputs": [],
      "source": [
        "for d in os.listdir('data/'):\n",
        "    for f in os.listdir('data/' + d):\n",
        "        os.remove('data/' + d + '/' + f)\n",
        "    os.rmdir('data/' + d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "7627fb6d-5c46-4136-9afc-da2d8dc79b26",
      "metadata": {
        "id": "7627fb6d-5c46-4136-9afc-da2d8dc79b26"
      },
      "outputs": [],
      "source": [
        "os.rmdir('data')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baf393ec-6c3e-4846-b752-a05c754addf9",
      "metadata": {
        "id": "baf393ec-6c3e-4846-b752-a05c754addf9"
      },
      "source": [
        "### Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fc9ee40-7cb8-460a-8a28-782d7967554c",
      "metadata": {
        "id": "5fc9ee40-7cb8-460a-8a28-782d7967554c"
      },
      "source": [
        "1. `keras.applications` offers plenty of choice for pre-trained models, beyond VGG16. `https://keras.io/api/applications` can help you to choose. For instance, you can try **Xception**, which uses some additional tricks proposed by F Chollet.\n",
        "\n",
        "2. You can easily unfreeze some of the last layers of the pre-trained model. For instance, in the VGG16 model, after freezing all the layers with `conv_base.trainable = False`, you can apply the loop `for l in conv_base.layers[-2]: l.trainable = True`. Try this with the VGG16 model, to see whether you can improve the performance. For the fitting process to work, you will have to decrease the **learning rate** (the default is `learning_rate=1e-3`). First, import the module `optimizers` (`from keras import optimizers`), and then compile the model using the argument `optimizer=optimizers.Adam(learning_rate=5e-5)`.\n",
        "\n",
        "3. If you have survived to the preceding exercises, you can play with the learning rate, to see how this affects the fitting process."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}