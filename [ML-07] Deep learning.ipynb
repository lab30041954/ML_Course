{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lab30041954/ML_IESE_Course/blob/main/%5BML-07%5D%20Deep%20learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e77f92-90b6-44e5-88cd-40d8dac548f2",
      "metadata": {
        "id": "63e77f92-90b6-44e5-88cd-40d8dac548f2"
      },
      "source": [
        "# [ML-07] Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea3885a-56c7-499b-bfc9-3560e731c53b",
      "metadata": {
        "id": "eea3885a-56c7-499b-bfc9-3560e731c53b"
      },
      "source": [
        "## What is deep learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9567513-506c-44a8-acaf-ef6324636613",
      "metadata": {
        "id": "e9567513-506c-44a8-acaf-ef6324636613"
      },
      "source": [
        "**Deep learning**, the current star of machine learning, is based on neural networks. The success of deep learning, not yet fully understood, is attributed to the ability of creating improved **representations** of the input data by means of successive layers of features.\n",
        "\n",
        "Under this perspective, deep learning is a successful approach to **feature engineering**. Why is this needed? Because, in many cases, the available features do not provide an adequate representation of the data. So, replacing the original features by a new set may be useful. At the price of oversimplifying a complex question, the following two examples may help to understand this:\n",
        "\n",
        "* A **pricing model** for predicting the sale price of a house from features like the square footage of the plot and the house, the location, the number of bedrooms, the existence of a garage, etc. You will probably agree that these are, indeed, the features that determine the price, so they provide a good representation of the data, and a **shallow learning** model, such as a gradient boosting regressor, would be a good approach. No feature engineering is needed here.\n",
        "\n",
        "* A model for **image classification**. Here, the available features are based on a grid of pixels. But we do not recognize images from specific pixel positions. Recognition is based on **shapes** and **corners**. A shape is a created by a collection of pixels, each of them close to the preceding one. And a corner is created by two shapes intersecting in a specific way. This suggests that a neural network with an input layer of pixels, a first hidden layer of shapes, and a second layer of corners can provide a better representation, useful for image classification.\n",
        "\n",
        "The number of hidden layers in a neural network is called the **depth**. But, although deep learning is based on neural networks with more than one hidden layer, there is more in deep learning than additional layers. In the MLP model as we have seen it, every hidden node is connected to all the nodes of the preceding layer and all the nodes of the following layer. In the deep learning context, these fully-connected layers are called **dense**. But there are other types of layers, and the most glamorous applications of deep learning are based on networks which are not fully-connected."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0d2c5b-4afb-42bb-8090-de17b47bf816",
      "metadata": {
        "id": "dd0d2c5b-4afb-42bb-8090-de17b47bf816"
      },
      "source": [
        "## Convolutional neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac24109-39b9-47d2-b65b-5f5c0782ad3d",
      "metadata": {
        "id": "7ac24109-39b9-47d2-b65b-5f5c0782ad3d"
      },
      "source": [
        "In the classic MLP network, hidden and output layers were dense, that is, every node was connected to all neurons in the next layer. A **convolutional neural network** (CNN) contains other types of layer, such as **convolutional layers** and **max pooling layers**. These layers have low connectivity, and the connections are selected according a design which takes advantage of the hierarchical pattern in the data. The main idea is that dense layers learn global patterns in their input feature space (*e.g*. in the MNIST data, patterns involving all pixels), while these new layers learn local patterns, *i.e*. patterns found in small 1D or 2D windows of the inputs.\n",
        "\n",
        "There are two subtypes of convolutional networks:\n",
        "\n",
        "* **1D convolutional networks** (Conv1D), used with sequence data, such as text. Though they were relevant ten years ago, they have been left aside, due to the superior performance of the **transformer networks** used in **large language models**.\n",
        "\n",
        "* **2D convolutional networks** (Conv2D), used in image classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a4e72d-0e86-4728-8cd1-dbfacd9568e9",
      "metadata": {
        "id": "25a4e72d-0e86-4728-8cd1-dbfacd9568e9"
      },
      "source": [
        "## Applications to computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6669b3b5-dea7-4ec2-81fc-61ef20bc1580",
      "metadata": {
        "id": "6669b3b5-dea7-4ec2-81fc-61ef20bc1580"
      },
      "source": [
        "In the CNN's used in **image classification**, the input is a 3D tensor, called a **feature map**. The feature map has two spatial axes, called **height** and **width**, and a **depth** axis. For an RGB image, the dimension of the depth axis would be 3, since the image has 3 color channels, red, green, and blue. For grayscale images like those of the MNIST digits, it is just 1 (the gray levels).\n",
        "\n",
        "The basic innovation is the `Conv2D` layer, which extracts patches from its input feature map, typically with a 3 $\\times$ 3 window, applying the same transformation to all of these patches, producing a new output feature map. This output feature map is still a 3D tensor: it has width, height and depth. Its depth can be arbitrary, since the output depth is a parameter of the layer, and the different channels in that depth axis no longer stand for specific colors like in an RGB input, but for different views of the input, called **filters**. The filters encode specific aspects of the input data. For instance, at a high level, a single filter could be encoding the concept \"presence of a face in the input\".\n",
        "\n",
        "With convolutional networks, practitioners typically use two strategies for extracting more of their data:\n",
        "\n",
        "* **Transfer learning**. Instead of starting to train your model with random coefficients, you start with those of a model which has been pre-trained with other data. There is plenty of supply of pre-trained models, as we will comment in lecture ML-20.\n",
        "\n",
        "* **Data augmentation**. Expanding the training data with images obtained by transforming the original images. Typical transformations are: rotation with a random angle, random shift and zoom. Keras offers many resources for that, though we don't have room for them in this short course."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a7ab82-25b1-4aae-811b-f4f1eac900ec",
      "metadata": {
        "id": "a5a7ab82-25b1-4aae-811b-f4f1eac900ec"
      },
      "source": [
        "## CNN models in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "514bd6fa-4c3d-4486-a9e7-03c37cfc8e61",
      "metadata": {
        "id": "514bd6fa-4c3d-4486-a9e7-03c37cfc8e61"
      },
      "source": [
        "Let us use again the MNIST data as to illustrate the Keras syntax, now for CNN models. The height and the width are 28, and the depth is 1. We start by reshaping the training and test feature matrices as 3D arrays, so they can provide inputs for a `Conv2D` layer:\n",
        "\n",
        "```\n",
        "X_train, X_test = X_train.reshape(60000, 28, 28, 1), X_test.reshape(10000, 28, 28, 1)\n",
        "```\n",
        "\n",
        "*Note*. This reshaping may not be needed if you get the MINST data from other sources than the GitHub repository of this course.\n",
        "\n",
        "In the Functional API, the network architecture is specified as a sequence of transformations. We have seen this in example ML-15. The following architecture has been taken from a Keras example:\n",
        "\n",
        "```\n",
        "input_tensor = Input(shape=(28, 28, 1))\n",
        "x1 = layers.Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "x2 = layers.MaxPooling2D((2, 2))(x1)\n",
        "x3 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "x4 = layers.MaxPooling2D((2, 2))(x3)\n",
        "x5 = layers.Conv2D(64, (3, 3), activation='relu')(x4)\n",
        "x6 = layers.Flatten()(x5)\n",
        "x7 = layers.Dense(64, activation='relu')(x6)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x7)\n",
        "```\n",
        "\n",
        "A summary of the network can be printed with the method `.summary()`. In this case, we woud get the table:\n",
        "\n",
        "```\n",
        "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
        "┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃\n",
        "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
        "│ input_layer_3 (InputLayer)      │ (None, 28, 28, 1)      │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_6 (Conv2D)               │ (None, 26, 26, 32)     │           320 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_6 (MaxPooling2D)  │ (None, 13, 13, 32)     │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_7 (Conv2D)               │ (None, 11, 11, 64)     │        18,496 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ max_pooling2d_7 (MaxPooling2D)  │ (None, 5, 5, 64)       │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ conv2d_8 (Conv2D)               │ (None, 3, 3, 64)       │        36,928 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ flatten_2 (Flatten)             │ (None, 576)            │             0 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_6 (Dense)                 │ (None, 64)             │        36,928 │\n",
        "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
        "│ dense_7 (Dense)                 │ (None, 10)             │           650 │\n",
        "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
        "```\n",
        "\n",
        "The table indicates, for every layer, the output shape and the number of parameters involved. The first layer is a `Conv2D` layer of 32 nodes. Every node takes data from a 3 $\\times$ 3 window (submatrix) of the 28 $\\times$ 28 pixel matrix, performing a convolution operation on those data. There are 26 $\\times$ 26 such windows, so the output feature map will have height and width 26. The convolution is a linear function of the input data. For a specific node, the coefficients used by the convolution are the same for all windows.\n",
        "\n",
        "`Conv2D` layers are typically alternated with `MaxPooling2D` layers. These layers also use windows (here 2 $\\times$ 2 windows), from which they just extract the maximum value (no parameters needed). In the `MaxPooling2D` layer, the windows are disjoint, so the size of the feature map is halved. Therefore, the output feature map has height and width 13. We have an output feature map for every input feature map.\n",
        "\n",
        "We continue with two `Conv2D` layers, with 64 nodes each, with a `MaxPooling2D` layer in-between. The output is now a tensor of shape `(3, 3, 64)`. The network is closed by a stack of two `Dense` layers. Since the input in the first of these layers has to be a one-dimensional, we have to flatten the 3D output of the last `Conv2D` layer to a 1D tensor. This is done with a `Flatten` layer, which involves no calculation, being just a reshape.\n",
        "\n",
        "Next, we initialize the class `model.Models()`, specifying the input and the output:\n",
        "\n",
        "```\n",
        "clf = models.Models(input_tensor, output_tensor)\n",
        "```\n",
        "\n",
        "Now we can apply, as in the MLP model of lecture ML-16, the methods `.compile()`, `.fit()` and `.evaluate()`:\n",
        "\n",
        "```\n",
        "clf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "clf.fit(X_train, y_train, epochs=10)\n",
        "clf.evaluate(X_test, y_test)\n",
        "```\n",
        "\n",
        "Alternatively, you can fit and evaluate the model in one shot, testing after every epoch:\n",
        "\n",
        "```\n",
        "clf.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b40921e3-cf9e-40b9-b4ff-faabe94eccfe",
      "metadata": {
        "id": "b40921e3-cf9e-40b9-b4ff-faabe94eccfe"
      },
      "source": [
        "## Applications to sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87613649-827f-40d1-8be3-d1aa846aa590",
      "metadata": {
        "id": "87613649-827f-40d1-8be3-d1aa846aa590"
      },
      "source": [
        "The second area of success of deep learning is **sequence data**. This is a generic expression including text, time series data, sound and video. You may find many sources about to use 1D convolutional networks and **recurrent neural networks** (RNN) to this type of data.\n",
        "\n",
        "Though this course is also concerned with the applications of deep learning to text data, it does not cover the use of CNN and RNN models to text data, since it is gettting obsolete, due to the strong push of the large language models. So, we postpone text data analysis until we have introduced the new toolkit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca25cf83-8b19-4e75-b798-ebb14de2bcea",
      "metadata": {
        "id": "ca25cf83-8b19-4e75-b798-ebb14de2bcea"
      },
      "source": [
        "## Example - The MNIST data (2nd round)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abd8aca0-a3f0-4694-962c-f56c9cb082d0",
      "metadata": {
        "id": "abd8aca0-a3f0-4694-962c-f56c9cb082d0"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a6e3c8-da00-4dc3-b364-71f815bb420b",
      "metadata": {
        "id": "58a6e3c8-da00-4dc3-b364-71f815bb420b"
      },
      "source": [
        "We come back to the MNIST data of the preceding lecture. In this lecture, we train a deeper model, to explore the extent to which we can improve our previous results.\n",
        "\n",
        "*Note*. If you run this notebook in Google Colab, you can find things a bit slow. You can speed up everything by chan ging the runtime to"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9435c7-8773-47cd-99d7-ba92f0a56b8d",
      "metadata": {
        "id": "ef9435c7-8773-47cd-99d7-ba92f0a56b8d"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fce14d5-66ae-4d57-9697-2bd713bb2d17",
      "metadata": {
        "id": "1fce14d5-66ae-4d57-9697-2bd713bb2d17"
      },
      "source": [
        "Q1. Rerun the part of the code used in the previous lecture that is needed to obtain the rescaled features matrices `X_train` and `X_test` and the target vectors `y_train` and `y_test`.\n",
        "\n",
        "Q2. Train an MLP model, to be used as a benchmark.\n",
        "\n",
        "Q3. Try now with a convolutional neural network. Do we get a real improvement with this *deep* model?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59855dfa-f071-48ce-9043-1d0605f2960f",
      "metadata": {
        "id": "59855dfa-f071-48ce-9043-1d0605f2960f"
      },
      "source": [
        "### Q1. Recovering the rescaled MNIST data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce73a08b-64b7-4d11-86dd-b991858ac116",
      "metadata": {
        "id": "ce73a08b-64b7-4d11-86dd-b991858ac116"
      },
      "source": [
        "With a collection of code lines extracted from those used in the preceding lecture, we get `X_train` and `X_test`, `y_train` and `y_test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70c6077f-acc7-4376-9f0e-f876f068d3a1",
      "metadata": {
        "id": "70c6077f-acc7-4376-9f0e-f876f068d3a1"
      },
      "outputs": [],
      "source": [
        "import numpy as np, pandas as pd\n",
        "path = 'https://raw.githubusercontent.com/lab30041954/Data/main/'\n",
        "df = pd.read_csv(path + 'mnist.csv.zip')\n",
        "y = df['label'].values\n",
        "X = df.drop(columns='label').values/255\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "984c473f-c85e-4887-8c65-15debe7f0d29",
      "metadata": {
        "id": "984c473f-c85e-4887-8c65-15debe7f0d29"
      },
      "source": [
        "## Q2. MLP model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e53ec8f-f7a1-4a04-aa41-418fd3edcba5",
      "metadata": {
        "id": "8e53ec8f-f7a1-4a04-aa41-418fd3edcba5"
      },
      "source": [
        "We train again the MLP model of the preceding lecture. Ten epochs will be enough, based on that experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7b4fd396-6445-4ea9-a3cd-b01d97b53d72",
      "metadata": {
        "id": "7b4fd396-6445-4ea9-a3cd-b01d97b53d72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342e4fa6-d6a8-4ae5-b5cb-b94ee1e1b3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - acc: 0.8428 - loss: 0.5611 - val_acc: 0.9283 - val_loss: 0.2447\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9406 - loss: 0.2073 - val_acc: 0.9435 - val_loss: 0.1918\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - acc: 0.9548 - loss: 0.1618 - val_acc: 0.9501 - val_loss: 0.1662\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9612 - loss: 0.1322 - val_acc: 0.9562 - val_loss: 0.1464\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9666 - loss: 0.1157 - val_acc: 0.9602 - val_loss: 0.1358\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - acc: 0.9710 - loss: 0.0976 - val_acc: 0.9576 - val_loss: 0.1382\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9730 - loss: 0.0894 - val_acc: 0.9599 - val_loss: 0.1345\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - acc: 0.9761 - loss: 0.0792 - val_acc: 0.9619 - val_loss: 0.1236\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9784 - loss: 0.0737 - val_acc: 0.9650 - val_loss: 0.1201\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - acc: 0.9795 - loss: 0.0699 - val_acc: 0.9647 - val_loss: 0.1206\n"
          ]
        }
      ],
      "source": [
        "from keras import Input, models, layers\n",
        "input_tensor = Input(shape=(784,))\n",
        "x = layers.Dense(32, activation='relu')(input_tensor)\n",
        "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
        "mlpclf = models.Model(input_tensor, output_tensor)\n",
        "mlpclf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "mlpclf.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75ebf79b-18ab-4864-b4c9-aa86529a46ca",
      "metadata": {
        "id": "75ebf79b-18ab-4864-b4c9-aa86529a46ca"
      },
      "source": [
        "### Q3. CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "450e4c1f-6f02-4c7a-b86f-c09f06b269cd",
      "metadata": {
        "id": "450e4c1f-6f02-4c7a-b86f-c09f06b269cd"
      },
      "source": [
        "We dig deeper now, exploring **convolutional neural network** (CNN) models, in particular a model based on a 2D convolutional network. Instead of using a 60,000 $\\times$ 784 features matrix (a 2D array), we pack the training data as a 4D array, in which the axes are:\n",
        "\n",
        "* `axis=0` with dimension 60,000, identifies the inputs, *i.e*. the images,\n",
        "\n",
        "* `axis=1` and `axis=2` both with dimension 28, identify the pixel positions.\n",
        "\n",
        "* `axis=2` with 1 dimension, identifies the **channel**. Since we work with gray scale, there is only one channel (for RGB pictures, we would have three channels).\n",
        "\n",
        "We reshape the arrays `X_train` and `X_test` as 4D arrays accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e028120a-fa95-4882-81ed-8be35907ebad",
      "metadata": {
        "id": "e028120a-fa95-4882-81ed-8be35907ebad"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train.reshape(60000, 28, 28, 1), X_test.reshape(10000, 28, 28, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34f354b3-3cb5-4e93-96e2-fe88af9423c2",
      "metadata": {
        "id": "34f354b3-3cb5-4e93-96e2-fe88af9423c2"
      },
      "source": [
        "As for the MLP model, we specify the network architecture as a sequence of transformations. In this example, we propose a standard CNN architecture, used by many authors for these data.\n",
        "\n",
        "In this case, every input will be 3D array, with shape `(28,28,1)`. The networks starts with a sequence of three `Conv2D` layers, with 32, 64 and 64 nodes, respectively, with two interspersed `MaxPooling` layers. After passing these layers, the input has been transformed into a set of 64 smaller 3D arrays (we will see below how small). Then, this set is flattened into a 1D array, which is the input for the rest of the network. This last part part is the same as a MLP network with a hidden layer of 64 nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "c1872fbd-41bb-4748-a5f3-2932cfb2dbf2",
      "metadata": {
        "id": "c1872fbd-41bb-4748-a5f3-2932cfb2dbf2"
      },
      "outputs": [],
      "source": [
        "input_tensor = Input(shape=(28, 28, 1))\n",
        "    ...: x1 = layers.Conv2D(32, (3, 3), activation='relu')(input_tensor)\n",
        "    ...: x2 = layers.MaxPooling2D((2, 2))(x1)\n",
        "    ...: x3 = layers.Conv2D(64, (3, 3), activation='relu')(x2)\n",
        "    ...: x4 = layers.MaxPooling2D((2, 2))(x3)\n",
        "    ...: x5 = layers.Conv2D(64, (3, 3), activation='relu')(x4)\n",
        "    ...: x6 = layers.Flatten()(x5)\n",
        "    ...: x7 = layers.Dense(64, activation='relu')(x6)\n",
        "    ...: output_tensor = layers.Dense(10, activation='softmax')(x7)\n",
        "    ...: cnnclf= models.Model(input_tensor, output_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce5af77a-cca6-4eae-804b-2fc7cd2a3a31",
      "metadata": {
        "id": "ce5af77a-cca6-4eae-804b-2fc7cd2a3a31"
      },
      "source": [
        "As for the MLP model, we can print a summary reporting the number of parameters involved in every layer:\n",
        "\n",
        "* The first `Conv2D` layer has 32 nodes, with specific parameters. At every node, a special type of linear calculation, called **convolution**, takes place. The convolution extracts a single number of a 3 $\\times$ 3 submatrix, using nine weights and one bias. This makes a total of 10 parameters for every node, a grand total of 32 $\\times$ 10 = 320 parameters. The original 28 $\\times$ 28 matrix can be covered by a 26 $\\times$ 26 grid of 3 $\\times$ 3 submatrices, so the outputs of this layer have shape (26, 26, 32).\n",
        "\n",
        "* The `MaxPooling` layer uses 2 $\\times$ 2 submatrices and, instead of a convolution, it just takes the maximum value. So, no parameters are involved, and the outputs have shape (13, 13, 32).\n",
        "\n",
        "* The third layer has 64 nodes. Every node takes inputs from the 32 nodes of the preceding layer. Nine weights are needed for every input and one bias for the whole set, making a total of 32 $\\times$ 9 + 1 = 289 parameters for every node. So, we have 64 $\\times$ 289 = 18,496 additional parameters in this layer. Now, the outputs have shape `(11, 11, 64)`.\n",
        "\n",
        "* The next two layers can be explained in a similar way. In the fifth layer, we have added 36,928 parameters and the outputs have shape `(3, 3, 64)`.\n",
        "\n",
        "* The sixth layer consistes in taking 3 $\\times$ 3 $\\times$ 64 = 576 inputs and arranging them as a 1D array. No parameters are needed.\n",
        "\n",
        "* The seventh layer is a dense layer, so have we already know how it works. The 64 nodes need 577 parameters each to manage the 576 inputs. So, we have 64 $\\times$ 577 = 36,928 additional parameters, and the output has shape `(64,)`.\n",
        "\n",
        "* The last layer has 10 nodes. Each node takes 64 inputs, involving 65 parameters. So we have 10 $\\times$ 65 = 650 additional parameters, ad the output is a vector of 10 class probabilities.\n",
        "\n",
        "At the end of the day, comparing this *deep* model with the previous MLP model, with only 32 hidden nodes, the number of parameters does not increase so much. This is due to the **low connectivity** of the CNN model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "595c5ca9-eebd-437d-b83e-42d691df1645",
      "metadata": {
        "id": "595c5ca9-eebd-437d-b83e-42d691df1645",
        "outputId": "66520698-4faa-477b-8d40-6af5019d7336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m1\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m93,322\u001b[0m (364.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">93,322</span> (364.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "cnnclf.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6b40626-6f9a-41e3-9c61-67ad05f563fa",
      "metadata": {
        "id": "c6b40626-6f9a-41e3-9c61-67ad05f563fa"
      },
      "source": [
        "The rest of the process is the same as in the MLP models. We use here `epochs=10`, to make it shorter. At the end of the first epoch, the model achieves 97.5% accuracy on the test data. After five epochs, this has been raised to 99%, which is quite satisfactory, and the improvement stops there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3fc801e2-3c5a-401b-8b4b-80395f7c9fc4",
      "metadata": {
        "id": "3fc801e2-3c5a-401b-8b4b-80395f7c9fc4",
        "outputId": "796f3db2-b348-4ec8-95a2-51e7b65296c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - acc: 0.8973 - loss: 0.3301 - val_acc: 0.9757 - val_loss: 0.0735\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - acc: 0.9851 - loss: 0.0469 - val_acc: 0.9878 - val_loss: 0.0397\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - acc: 0.9893 - loss: 0.0332 - val_acc: 0.9882 - val_loss: 0.0386\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - acc: 0.9918 - loss: 0.0253 - val_acc: 0.9864 - val_loss: 0.0459\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - acc: 0.9940 - loss: 0.0177 - val_acc: 0.9892 - val_loss: 0.0355\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - acc: 0.9949 - loss: 0.0155 - val_acc: 0.9909 - val_loss: 0.0317\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - acc: 0.9966 - loss: 0.0114 - val_acc: 0.9899 - val_loss: 0.0413\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - acc: 0.9967 - loss: 0.0099 - val_acc: 0.9909 - val_loss: 0.0368\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - acc: 0.9975 - loss: 0.0077 - val_acc: 0.9906 - val_loss: 0.0337\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - acc: 0.9983 - loss: 0.0052 - val_acc: 0.9908 - val_loss: 0.0415\n"
          ]
        }
      ],
      "source": [
        "cnnclf.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "cnnclf.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test));"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}