{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81699508-1db3-431e-981b-317d4b285a59",
   "metadata": {},
   "source": [
    "# [ML-10] Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055db03-d3ef-4576-9bdc-33702286c74c",
   "metadata": {},
   "source": [
    "## What is natural language processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b3919f-b08f-44f4-961c-3a1448937f2a",
   "metadata": {},
   "source": [
    "**Natural language processing** (NLP) is an interdisciplinary subfield of computer science and artificial intelligence. It is concerned with providing computers the ability to process text written in a natural language. \n",
    "\n",
    "The adjective *natural* is used here to refer to the languages we humans use, such as English, Spanish, etc, as opposed to computer languages like Python or Java. Nevertheless, the language models developed since 2020 have been trained in a way that they can process code written in a number of computer languages, so the distinction is fading away.\n",
    "\n",
    "NLP has advanced in giant steps in the last years, as **large language models** (LLM's) took the stage. Unfortunatley, this means that reading anything published ealier than five years ago may be wasting time. \n",
    "\n",
    "The NLP toolkit used in this course is based on LLM's. The input data unit is typically a string (which can be a long document). A data set contains a collection of these inputs. Some classic collections are called **corpora**. A famous one is the Wikipedia corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7ac591-56ed-418d-88a2-9a0ef336f8f0",
   "metadata": {},
   "source": [
    "## NLP tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bfe9f-eff9-4742-8233-45de44729d32",
   "metadata": {},
   "source": [
    "Natural language processing turned around a few specific tasks during many years. The classics are:\n",
    "\n",
    "* **Text classification**. The input is a text and the output a set of class probabilities. For instance, in the preceding lecture we trained a model for fake news detection. Another popular application, which appears in the example of this lecture, is **sentiment analysis**. \n",
    "\n",
    "* **Text generation**. The input is a text and the output another text. This covers many specific tasks:\n",
    "\n",
    "    + **Summarization**. The output text is a summarized version of the input text. \n",
    "\n",
    "    + **Translation**. The output text is a translation of the input text to another language. Automatic translation from Russian to English is one of the very classics of artificial intelligence.\n",
    "\n",
    "    + **Question answering** (QA). The input text is a question and the output text is an answer to that question, either extracted from a context text inputted together with the question, or retrieved from a data source.\n",
    "\n",
    "    + **Named entity recognition** (NER). The input is a text together with a collection of entities. The output is a list of the entities found in the text. For instance, the model can extract a dish name from a restaurant review.\n",
    "\n",
    "Though this view of natural language processing is still found in many places, the perspective has changed. Right now, we have, essentially, two types of models: the **embedding models**, that we have already seen in this course, and **text generation models**, which appear in this lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb79f62d-1b84-4cd5-a40b-4184974a5caf",
   "metadata": {},
   "source": [
    "## Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6a220-f2f6-4e90-ae3b-f8509b5b6a99",
   "metadata": {},
   "source": [
    "Even if the data unit in NLP is a string, that string is not processed as a whole, but previously split in a list of substrings called **tokens**. The extraction of tokens from the input text is called **tokenization**. Tokenization is one of the oldest problems in NLP, and different approaches have been discussed for years: words, sub-words, pairs of words, etc. Also, tokenization is a harder problem in some languages, like Spanish and French, with many verb forms, or in German, with its declensions, prefixes and suffixes, than in English.\n",
    "\n",
    "Nowadays, the debate about the tokenization level has lost interest for the practitioners). First, there is plenty of supply of tokenization models, called **tokenizers**. Second, all the relevant LLM's come with their own tokenizers, so you don't have to think about them once you have chosen your model. In these models, most of the tokens are words, and a small proportion of subwords. Also, punctuation marks can be tokens. In addition to all these, there is an \"unknown\" token (the vocabulary of the model is limited), a token for the start of the text and another token for the end. So, when we input a string to one of these models, it is converted to a list of tokens, though we never see them in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9387d-93c9-4f4b-9b09-333ce6cb80c5",
   "metadata": {},
   "source": [
    "## What are large language models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc32a83-3bf9-4ea5-a78c-bff14c2de145",
   "metadata": {},
   "source": [
    "Large language models are not only large (they often go beyond 1B parameters), but a new generation of neural network models. They are based on a novel architecture, the **transformer**, published in 2017. An LLM can be used directly, or taken as a **pre-trained model** for transfer learning, which in this context is called **fine-tuning**. Transfer learning is very common in LLM's. The original pre-trained models are called **foundation models**, and the task for which they are fine-tuned is called the **downstream task**.\n",
    "\n",
    "There are just a few relevant foundation models, but thousands of fine-tuned versions of them. You can find most of these in Hugging Face. The classic embedding model was **BERT** (Bidirectional Encoder Representations from Transformers), developed by Google, and the classic text generation model was **GPT** (Generative Pre-trained Transformer), developed by OpenAI. \n",
    "\n",
    "The models behind the chat apps (ChatGPT, Gemini, etc) are the text generation models. The input text is called the **prompt**. To get the best output, you have to carefully craft your prompts. This is called **prompt engineering**. Two simple examples: \n",
    "\n",
    "* To get a summary, the user includes in the prompt instructions such as *summarize the following text, in no more than 100 words* $\\dots$. \n",
    "\n",
    "* To get a translation, he/she includes something like *translate to French the following text* $\\dots$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941134d4-2c48-43f8-8a4a-f555ff521645",
   "metadata": {},
   "source": [
    "## What makes the transformers special?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5458b5-b5de-4e84-ba49-2d1a06ebac02",
   "metadata": {},
   "source": [
    "The first segment of a transformer is always a tokenizer, specific for the model. The tokenizer comes with a **token dictionary** and a list of embedding vectors of a given length, one for each token. When we prompt a string, the string is split in a list of tokens, and the corresponding vectors are packed as a matrix, with a token embedding vector in each row. So, the input in a transformer is, properly speaking, a 2D array, not a text. This implies that, even if the original transformer was designed with an NLP perspective, in the transformer, as in any model based on a neural network architecture, the input and the output are tensors. As a matter of fact, transformers have also been applied in computer vision (they are a serious challenge to CNN models).\n",
    "\n",
    "The three differential components of the transformer are briefly explained below.\n",
    "\n",
    "* At the beginning, a **positional embedding** are used to give the model information about the position of each token in the input text or, more specifically, the row number in the input 2D array. The positional embedding generates a different vector for every row. These vectors are added to the input token vector. This is a way of encoding the order of the tokens (and punctuation) in the text, which is one of the ingredients of the \"meaning\". \n",
    "\n",
    "* The mid part of the transformer is a combo of common **feedforweard layers** (those that compose an MLP network) and **attention layes**. The attention layer is a transformation that replaces every token vector by a weighted average of all token vectors. The weights are based on the similarity among the token vectors. A simple example may help to understand the role of attention. Suppose that word 'bank' comes in the sentence 'I got money from the bank'. The attention mechanism changes the vector that stands for 'bank' by pushing it toward the vector that stands for money. So, 'bank' gets a more \"financial\" meaning. This is the way in which the attention mechanism captures the meaning of a word from the context.\n",
    "\n",
    "* At the end, the output layer is the same as in the classification neural networks that we have seen in this course, but the predicted token is not always the one with the maximum probability. Instead it is randomly chosen according to the predicted class (token) probabilities. For instance, if the model predicts a probability 0.4 for a certain token, that token will be chosen 40% of the time. The softmax activation is also slightly modified, with an additional parameter called **temperature**. This controls the randomness of the prediction of the next token. In mathematical terms, the token probabilities are calculated as\n",
    "\n",
    "$$p_i = \\frac{\\exp(z_i/t)}{\\exp(z_1/t) + \\cdots + \\exp(z_m/t)}.$$\n",
    "\n",
    "With $t = 1$, the next token probabilities are calculated with the ordinary softmax function. With $t = 0$, the probability is equal to 1 for the token for which $z_i$ is maximum, and 0 for the rest. So, there is no randomness at all, and, for a given input, the model will predict always the same token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28bbdc-8d02-409a-94f2-bad8410dba8a",
   "metadata": {},
   "source": [
    "## Example - Tweet sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46d925d-c53a-4065-9241-77566d380a8e",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0382a10f-74ba-4bc7-9ff9-a96eafae7e0b",
   "metadata": {},
   "source": [
    "**Sentiment analysis** aims at capturing the writer's attitude towards a particular topic, product, etc. The simplest versions classify texts as Positive/Negative, or Neutral/Positive/Negative, while the most complex versions consider the many dimensions of the writer's attitude, such as joy, sadness, anger, or fear, and their intensity. \n",
    "\n",
    "**Large language models** (LLM) have been applied for sentiment analysis since the very beginning, with Twitter/X providing training data sets. The data set of this example, available from the Hugging Face, has been used many times for **fine tuning** foundation models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9c425f-e742-4604-98c0-500a728652c9",
   "metadata": {},
   "source": [
    "### The data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a8c61-d742-4197-9267-0e94a6173678",
   "metadata": {},
   "source": [
    "The data set `sentiment.csv` contain 14,640 tweets from airlines' customers for sentiment analysis. The columns are:\n",
    "\n",
    "* `text`, the text of the tweet.\n",
    "\n",
    "* `label`, a label indicating the sentiment class: 0 = 'neutral', 1 = 'positive', and 2 = 'negative'.\n",
    "\n",
    "* `idx`, a counter that can be used as an ID.\n",
    "\n",
    "Source: Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0192ee19-e73e-4e59-a31e-bc92aa20db0c",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5abf60-9c52-43c7-8f53-2452c7ee8a6a",
   "metadata": {},
   "source": [
    "Q1. Encode the tweets provided with the model `all-minilm`, as in the preceding lecture.\n",
    "\n",
    "Q2. Pack the embedding vectors generated and the labels as a features matrix and a target vector.\n",
    "\n",
    "Q3. Train a **logistic regression model** on the training data set, and validate it on the validation data set.\n",
    "\n",
    "Q4. LLM's can perform sentiment analysis by themselves, without extra post-training. Could the results of the logistic regression model be matched with that approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75ad19-fa79-4844-b6df-4194fd6dd42e",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2d6a30-6e41-4d16-b170-42d0a8813490",
   "metadata": {},
   "source": [
    "We import the data from the GitHub repo, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a841f8-7948-4d86-aeee-38b9f2bbdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path = 'https://raw.githubusercontent.com/lab30041954/Data/main/'\n",
    "df = pd.read_csv(path + 'sentiment.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161e288-78c8-4a39-8189-cb57082ab854",
   "metadata": {},
   "source": [
    "Next, we explore the data set with the method `.info()`. The data are complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "820ebf35-a69e-48a4-8e34-de19bf750afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    14640 non-null  object\n",
      " 1   label   14640 non-null  int64 \n",
      " 2   idx     14640 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 343.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89fe50d-9c0e-4073-ba21-27d0ce860b10",
   "metadata": {},
   "source": [
    "The column `text` contains the text that has to be classifed. Printing a few rows shows that some of the text entries may be challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c178fcf9-ccfb-4688-87bb-99567c2839b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"@VirginAmerica it's really aggressive to blas...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  idx\n",
       "0                @VirginAmerica What @dhepburn said.      0    0\n",
       "1  @VirginAmerica plus you've added commercials t...      1    1\n",
       "2  @VirginAmerica I didn't today... Must mean I n...      0    2\n",
       "3  \"@VirginAmerica it's really aggressive to blas...      2    3\n",
       "4  @VirginAmerica and it's a really big bad thing...      2    4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a49b05-3f64-4fc6-9e02-8d36ee9d8f04",
   "metadata": {},
   "source": [
    "Finally, we explore the potential **class imbalance**. The method `.value_counts()` shows that the class proportions vary from 62.7 (for the negatives) to 16.1% (for the positives). This level of imbalance could affect the ability of the model to deal with the two minority classes. This is not addressed in the analysis presented here, and is left for the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901c5b16-cd86-48cd-9973-72b074ea8118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "2    0.626913\n",
       "0    0.211680\n",
       "1    0.161407\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()/len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecaa25d-11cf-4578-a561-f98dcaeef84e",
   "metadata": {},
   "source": [
    "### Q1. Encoding the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d20ff0b-de36-4211-bd3b-63561d7e95f4",
   "metadata": {},
   "source": [
    "We encode the tweets in the same way as the news in the preceding lecture. First, we import the function `embed()`, from the `ollama` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f991021-e072-424d-962f-71a4917d2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d2e7e3-740d-4de0-bfcb-7ebc1adaf496",
   "metadata": {},
   "source": [
    "Now, we write the text data as a list, which is needed by `embed()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "566ac18f-d4c7-41e7-9716-653115414061",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea35ae-fe49-4740-ba88-30a811d4a79b",
   "metadata": {},
   "source": [
    "We generate the the embedding vectors with the same model as in the preceding lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bd00e7f-8fb7-4184-a084-722a1196478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = embed(model='all-minilm:33m', input=text).embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fe8eb0-333a-4788-bc88-9ba0eadfd21d",
   "metadata": {},
   "source": [
    "### Q2. Packing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ede6cea-3432-4ac9-a69c-a9c070ca6bfa",
   "metadata": {},
   "source": [
    "The embedding vectors are packed as the rows of a features matrix, while the column `label` provides then target vector. We check the shapes, which are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52f3423d-081c-4548-9f66-173e270879d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 384)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y, X = df['label'], np.array(embeds)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a6c645-a4ec-4fef-a6a2-eba7307ebe24",
   "metadata": {},
   "source": [
    "### Q3. Logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d7f46-ccf1-42f8-a527-9069d5ba8aeb",
   "metadata": {},
   "source": [
    "We develop the logistic regression model just as we have done in other lectures. Overfitting is not relevant, and the accuracy is an 83%, quite respectable for this type of data. Anyway, don't forget that the accuracy may be poorer in the minority classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "914ce2b5-9409-4d7b-abfe-9b546cc1a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.836, 0.829)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "round(clf.score(X_train, y_train), 3), round(clf.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662aa2c2-8e11-42bb-985b-979b64837ff1",
   "metadata": {},
   "source": [
    "### Q4. Text generation model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2207ec9-00d8-463a-b645-0b19d72220fc",
   "metadata": {},
   "source": [
    "Pulling a text generation model from the Ollama platform works the same as pulling an embedding model. You can do it in the shell or in a Jupyter app, including in the last case the quotation mark (`!`), to warn the app that the command is a shell command, not a Python command (`! ollama pull gemma3n:e4b`).\n",
    "\n",
    "For a text generation model, the appropriate `ollama` function is `chat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94313278-5a6c-4ce5-a4f8-f208cfe1b919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7090ef6b-18f6-4097-88d8-bb726e93e1c6",
   "metadata": {},
   "source": [
    "The inputs to the model are given as a **JSON object**, which in Python is managed as dictionary, with (at least) two keys:\n",
    "\n",
    "* The **role**, which is 'user' for the inputs and 'assistant' for the outputs. \n",
    "\n",
    "* The **content**, which, for an input, is what we usually call a **prompt**.\n",
    "\n",
    "Let us illustrate this with the first tweet from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60216fcc-b6ef-4d13-92af-09498eabc946",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "You are an AI assistant whose task is to perform a sentiment analysis of tweets related to US airline companies.\n",
    "You are expected to assign to the input text one the following three labels: 0 = neutral, 1 = positive, 2 = negative.\n",
    "Respond with a single numeric label, without providing an explanation.\n",
    "text: @VirginAmerica What @dhepburn said.\n",
    "label: \n",
    "'''\n",
    "input = [{'role': 'user', 'content': prompt}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e15598-1003-4d73-abd6-e3b78244a0e2",
   "metadata": {},
   "source": [
    "The function `chat()` has (at least) two arguments, for the model and the message, respectively. For a first approach, we use the model `gemma3n:e4b`. The Google's **Gemma 3n models** are designed for efficient execution on everyday devices such as laptops, tablets or phones. Mind that this model takes 7.5 GB, so this may run slow if you are short of RAM memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad022117-c7d4-45f7-9905-0f9a33019e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chat(model='gemma3n:e4b', messages= input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8432e-c7df-4f89-86cc-aac96a1d5b3b",
   "metadata": {},
   "source": [
    "The object returned by the function `chat()` contains the response of the model (a string) plus metadata, not discussed here. The response can be extracted by applying `.message.content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff28e120-7afa-4970-812b-8cca38695685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c1a220-5adc-4181-9c36-aeffd4cdb607",
   "metadata": {},
   "source": [
    "The tweet has been classified as neutral. So, it seems that this approach may work. We can get it cleaner by converting the response to an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5acb293-9994-46a7-8a09-4b757cb3d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(resp.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1810520b-5490-4f71-b858-d0cd41667c1e",
   "metadata": {},
   "source": [
    "To apply this approach massively, to a whole tweet collection, we separate the instruction from the text to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28a93685-ad88-4975-b727-cb6fecbfdd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '''\n",
    "You are an AI assistant whose task is to assign to the input text one the following six labels: 0 = neutral, 1 = positive, 2 = negative.\n",
    "Respond with a single numeric label, without providing an explanation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf8d2a-f03b-46c3-9e26-3e0f239dda81",
   "metadata": {},
   "source": [
    "Next, we create an appropriate function, which we call `sentiment()`, for this job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88a6f752-189f-41c0-9ad3-68890f600cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    prompt = instruction + '\\n' + '\\n' + text  \n",
    "    input = [{'role': 'user', 'content': prompt}]\n",
    "    resp = chat(model='gemma3n:e4b', messages= input)\n",
    "    label = int(resp.message.content)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b401cf-34f8-4105-84e9-58813977b4a2",
   "metadata": {},
   "source": [
    "Let us check it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5aeb5211-40da-48cc-9002-47cd4e23dbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment('@VirginAmerica What @dhepburn said.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bcfbcc-79e8-49af-9464-ce686ed6f1cc",
   "metadata": {},
   "source": [
    "As a hint to more serious testing, we apply this function to a random selection of 100 data units. Again, mind that this can be slow if you are short of RAM memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d5bb087-d8ef-42b0-9e24-6f06c135c54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.sample(n=100)\n",
    "y1 = df1['label'].iloc[:100]\n",
    "y1_pred = df1['text'].iloc[:100].apply(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b110a570-43f7-42be-ac05-b196d4a90f95",
   "metadata": {},
   "source": [
    "Now, the confusion matrix tells us how good our approach is with this small sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a25181f-e568-49a5-b6a4-b13b03f980b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15,  6,  3],\n",
       "       [ 1, 21,  1],\n",
       "       [ 3,  1, 49]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y1, y1_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4bc241-a299-40ff-beb1-a70d71d36e4f",
   "metadata": {},
   "source": [
    "So far, an 85% accuracy. Also, note that the model seems to fail more often with the neutral messages. We suggest some ways to continue the analysis in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9189cc-6c51-4f89-9a96-bbec59d8a593",
   "metadata": {},
   "source": [
    "### Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ddf400-f501-4470-8b4c-9a3c8804fc5c",
   "metadata": {},
   "source": [
    "1. Use a confusion matrix to examine how homogeneous is the accuracy of the logistic model across the six classes. What do you think?\n",
    "\n",
    "2. No attention was paid in this lecture to the obvious **class imbalance** of the data sets. Can you correct what you found in the previous question by either **undersampling** or **oversampling** the training data set?\n",
    "\n",
    "3. Can you get a better logistic regression model by replacing the embedding model `all-minilm:33m` by a bigger one (*e.g*. the model `granite-embedding:278m` suggested in the preceding lecture)?\n",
    "\n",
    "4. Can you get better results by replacing the logistuic regression model by an MLP model with one hidden layer?\n",
    "\n",
    "5. Can you get better results with a bigger text generation model? If you have enough RAM available (16 GB), you can explore the use of OpenAI's `gpt-oss:20b`, or Alibaba's `qwen3:14b` (mind that these are thinking models, so they will be slower and verbose). On a different direction, you can also explore something smaller, such as Google's `gemma3:4b` or Meta's `llama3.2:3b`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
