{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c0189a-9125-429c-bf32-80a06ffa0a79",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/lab30041954/ML_Course/blob/main/Assignments/Assignment%204%20-%20The%20prof%20solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de5f0a9-d3bb-4083-903b-792a6229e360",
   "metadata": {},
   "source": [
    "# Assignment 4 - The prof solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fa0382-7bf0-4c46-8b26-1ba4f0a8331d",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577aeeb4-d26f-499b-9d1f-171e7042b00d",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f1c03f-caa4-4c08-9485-910e6aebde78",
   "metadata": {},
   "source": [
    "Develop a logistic regression classifier for these data. Compare it with the other models that have appeared in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e03865-c0e4-4a01-b009-d039b7bbdf3d",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd14e71-9f70-4dd6-bfa5-b30f1d07b463",
   "metadata": {},
   "source": [
    "We start by creating the features matrix `X` and the target vector `y`. The following code chunk has already been used in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1a118ee-ee9d-430c-9573-afbb0f94a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "path = 'https://raw.githubusercontent.com/lab30041954/Data/main/'\n",
    "df = pd.read_csv(path + 'mnist.csv.zip')\n",
    "y = df['label'].values\n",
    "X = df.drop(columns='label').values\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f6d7c5-1b7a-4ba0-bc0d-4c6e6b43bb8d",
   "metadata": {},
   "source": [
    "We split the data as in lecture ML-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1357906-b987-4783-a172-c5e209f6abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b366-b13b-4494-a1ad-598605394d78",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3e19d-3957-49ed-b613-0cdc5588ddcc",
   "metadata": {},
   "source": [
    "We train the linear regression model. The prices predicted by this model will be used for its evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3bbfd2-ce53-4acf-ba93-b293522df82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94, 0.919)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logclf = LogisticRegression(max_iter=500)\n",
    "logclf.fit(X_train, y_train)\n",
    "round(logclf.score(X_train, y_train), 3), round(logclf.score(X_test, y_test), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb4af95-a6cc-4363-a8c3-353d7c5efe10",
   "metadata": {},
   "source": [
    "Comparing this model with the MLP model with one hidden layer, we find evidence that the hidden layer improves the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfac72-15a3-4198-a497-4426c2f57c74",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccb593b-65e5-40a2-bdd4-f68ff3859fc9",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a61ed5-c5cf-490d-932d-faa8c098bddf",
   "metadata": {},
   "source": [
    "Calculate a **confusion matrix** for the logistic regression model (dimension 10x10). Which is the best classified digit? Which is the main source of misclassification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85be1d-9a05-410d-9306-2c320a10e8cf",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1009b-14dc-46d2-b91b-6457555a4070",
   "metadata": {},
   "source": [
    "Recycling code that we have already used, we calculate a confusion matrix for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dc666b4-6185-496a-9366-96c58e5f783c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 964,    0,    2,    2,    1,   13,    8,    0,    6,    0],\n",
       "       [   0, 1114,    6,    2,    1,    5,    1,    3,    7,    2],\n",
       "       [   4,   16,  936,   15,   11,    2,   15,    9,   30,    2],\n",
       "       [   1,    5,   32,  894,    0,   29,    2,   13,   21,   16],\n",
       "       [   1,    2,    5,    1,  896,    2,   12,   11,    5,   27],\n",
       "       [   8,    3,    6,   30,    8,  753,   16,    5,   26,    8],\n",
       "       [   9,    2,    5,    0,   11,   15,  942,    1,    4,    0],\n",
       "       [   4,    4,   15,    2,   13,    4,    1,  976,    4,   41],\n",
       "       [   4,   16,    9,   23,    6,   25,   10,    2,  853,   15],\n",
       "       [   4,    5,    5,   10,   32,   10,    2,   32,    6,  863]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = logclf.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71324a16-b6bf-4d3f-91a7-65a3129018e2",
   "metadata": {},
   "source": [
    "By comparing the numbers across the rows of this matrix, we see that the model does not show the same performance for all digits. To operationalize the comparison, we can calculate a percentage of error in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eab866c7-013d-40fb-835a-a9d2c79d9bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.2,  2.4, 10. , 11.7,  6.9, 12.7,  4.8,  8.3, 11.4, 10.9])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = conf.sum(axis=1)\n",
    "right = np.diagonal(conf)\n",
    "wrong = total - right\n",
    "percent_wrong = 100*wrong/total\n",
    "percent_wrong.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ef11d-b37e-47eb-8d41-c07355f501a6",
   "metadata": {},
   "source": [
    "The best classified digit is the 1, and the worst classified digit is the 5.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78111a71-411c-4096-844f-1b5923bcd9ee",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9ba53-5491-44d1-abb5-cc6cec2cb59b",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dfb213-eade-490d-b92c-8edd321ed484",
   "metadata": {},
   "source": [
    "Try some variations of the MLP model presented in the example of this lecture. For instance, you may increase the number of nodes in the hidden layer to 64, or decrease it to 16. And/or add a second hidden layer. Et cetera. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa2eab4-386b-4d52-82b7-c679429323df",
   "metadata": {},
   "source": [
    "### Linear regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4d7bbe-ab7c-42fc-92dc-9cd79574f1f0",
   "metadata": {},
   "source": [
    "Let us try a network with a single hidden layer, but changing the number of hidden nodes. The code is practically the same as in lecture ML-06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98e28e9b-fb8d-4f0e-afe5-e60b28b40f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.13/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 515us/step - acc: 0.9163 - loss: 0.2964 - val_acc: 0.9437 - val_loss: 0.1925\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - acc: 0.9585 - loss: 0.1428 - val_acc: 0.9578 - val_loss: 0.1403\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 404us/step - acc: 0.9693 - loss: 0.1037 - val_acc: 0.9630 - val_loss: 0.1246\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 405us/step - acc: 0.9759 - loss: 0.0816 - val_acc: 0.9676 - val_loss: 0.1100\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - acc: 0.9801 - loss: 0.0655 - val_acc: 0.9693 - val_loss: 0.0987\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step - acc: 0.9825 - loss: 0.0555 - val_acc: 0.9708 - val_loss: 0.0970\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - acc: 0.9857 - loss: 0.0466 - val_acc: 0.9715 - val_loss: 0.0948\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 403us/step - acc: 0.9874 - loss: 0.0409 - val_acc: 0.9712 - val_loss: 0.1000\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - acc: 0.9892 - loss: 0.0348 - val_acc: 0.9716 - val_loss: 0.1007\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 402us/step - acc: 0.9904 - loss: 0.0307 - val_acc: 0.9709 - val_loss: 0.1062\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step - acc: 0.9918 - loss: 0.0270 - val_acc: 0.9703 - val_loss: 0.1098\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 319us/step - acc: 0.9934 - loss: 0.0225 - val_acc: 0.9747 - val_loss: 0.0968\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - acc: 0.9933 - loss: 0.0215 - val_acc: 0.9726 - val_loss: 0.1159\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - acc: 0.9943 - loss: 0.0186 - val_acc: 0.9738 - val_loss: 0.1035\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 385us/step - acc: 0.9954 - loss: 0.0156 - val_acc: 0.9739 - val_loss: 0.1128\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - acc: 0.9958 - loss: 0.0144 - val_acc: 0.9746 - val_loss: 0.1115\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 377us/step - acc: 0.9963 - loss: 0.0125 - val_acc: 0.9742 - val_loss: 0.1191\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - acc: 0.9961 - loss: 0.0120 - val_acc: 0.9735 - val_loss: 0.1242\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - acc: 0.9963 - loss: 0.0115 - val_acc: 0.9732 - val_loss: 0.1245\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 422us/step - acc: 0.9972 - loss: 0.0095 - val_acc: 0.9748 - val_loss: 0.1262\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'jax'\n",
    "from keras import Input, models, layers\n",
    "input_tensor = Input(shape=(784,))\n",
    "x = layers.Dense(64, activation='relu')(input_tensor)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x)\n",
    "mlpclf1 = models.Model(input_tensor, output_tensor)\n",
    "mlpclf1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "mlpclf1.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32d008-83cc-4f84-a04a-852d603f0d6c",
   "metadata": {},
   "source": [
    "At the end of the day, this seems to be a bit better than the network trained in class. Let us try now with two hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650da9e5-01de-40fb-b691-09b512da90e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 446us/step - acc: 0.9031 - loss: 0.3376 - val_acc: 0.9408 - val_loss: 0.2024\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 382us/step - acc: 0.9545 - loss: 0.1519 - val_acc: 0.9543 - val_loss: 0.1455\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - acc: 0.9668 - loss: 0.1124 - val_acc: 0.9624 - val_loss: 0.1265\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 373us/step - acc: 0.9732 - loss: 0.0894 - val_acc: 0.9655 - val_loss: 0.1159\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 370us/step - acc: 0.9782 - loss: 0.0731 - val_acc: 0.9669 - val_loss: 0.1130\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 371us/step - acc: 0.9802 - loss: 0.0618 - val_acc: 0.9707 - val_loss: 0.1045\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - acc: 0.9836 - loss: 0.0522 - val_acc: 0.9649 - val_loss: 0.1226\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - acc: 0.9849 - loss: 0.0476 - val_acc: 0.9705 - val_loss: 0.1035\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 372us/step - acc: 0.9872 - loss: 0.0403 - val_acc: 0.9687 - val_loss: 0.1117\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - acc: 0.9882 - loss: 0.0359 - val_acc: 0.9722 - val_loss: 0.1042\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - acc: 0.9901 - loss: 0.0311 - val_acc: 0.9715 - val_loss: 0.1092\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 376us/step - acc: 0.9907 - loss: 0.0280 - val_acc: 0.9730 - val_loss: 0.1140\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - acc: 0.9913 - loss: 0.0254 - val_acc: 0.9712 - val_loss: 0.1212\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - acc: 0.9919 - loss: 0.0233 - val_acc: 0.9737 - val_loss: 0.1135\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 414us/step - acc: 0.9933 - loss: 0.0194 - val_acc: 0.9723 - val_loss: 0.1185\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 375us/step - acc: 0.9941 - loss: 0.0191 - val_acc: 0.9702 - val_loss: 0.1225\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - acc: 0.9945 - loss: 0.0166 - val_acc: 0.9703 - val_loss: 0.1246\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374us/step - acc: 0.9945 - loss: 0.0151 - val_acc: 0.9711 - val_loss: 0.1349\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - acc: 0.9950 - loss: 0.0150 - val_acc: 0.9723 - val_loss: 0.1302\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 380us/step - acc: 0.9947 - loss: 0.0159 - val_acc: 0.9732 - val_loss: 0.1332\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(784,))\n",
    "x1 = layers.Dense(64, activation='relu')(input_tensor)\n",
    "x2 = layers.Dense(16, activation='relu')(x1)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x2)\n",
    "mlpclf2 = models.Model(input_tensor, output_tensor)\n",
    "mlpclf2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "mlpclf2.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11940c57-7949-4a69-9f3a-f3c72e88369d",
   "metadata": {},
   "source": [
    "Not much progress with this second layer. Let us increase the number of hidden nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce73d013-cb82-43bf-bd72-510d280895e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 543us/step - acc: 0.9265 - loss: 0.2563 - val_acc: 0.9571 - val_loss: 0.1401\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 459us/step - acc: 0.9660 - loss: 0.1111 - val_acc: 0.9673 - val_loss: 0.1129\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447us/step - acc: 0.9769 - loss: 0.0764 - val_acc: 0.9678 - val_loss: 0.1075\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 448us/step - acc: 0.9817 - loss: 0.0575 - val_acc: 0.9711 - val_loss: 0.1008\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 443us/step - acc: 0.9858 - loss: 0.0451 - val_acc: 0.9739 - val_loss: 0.0956\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 447us/step - acc: 0.9878 - loss: 0.0366 - val_acc: 0.9733 - val_loss: 0.0986\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 489us/step - acc: 0.9899 - loss: 0.0300 - val_acc: 0.9747 - val_loss: 0.0994\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438us/step - acc: 0.9918 - loss: 0.0245 - val_acc: 0.9739 - val_loss: 0.1004\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - acc: 0.9926 - loss: 0.0212 - val_acc: 0.9741 - val_loss: 0.1065\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 438us/step - acc: 0.9933 - loss: 0.0194 - val_acc: 0.9760 - val_loss: 0.1076\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step - acc: 0.9939 - loss: 0.0179 - val_acc: 0.9732 - val_loss: 0.1175\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 437us/step - acc: 0.9950 - loss: 0.0143 - val_acc: 0.9741 - val_loss: 0.1233\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step - acc: 0.9948 - loss: 0.0153 - val_acc: 0.9759 - val_loss: 0.1163\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 436us/step - acc: 0.9959 - loss: 0.0131 - val_acc: 0.9738 - val_loss: 0.1318\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 439us/step - acc: 0.9963 - loss: 0.0103 - val_acc: 0.9743 - val_loss: 0.1284\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 440us/step - acc: 0.9960 - loss: 0.0120 - val_acc: 0.9742 - val_loss: 0.1420\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444us/step - acc: 0.9968 - loss: 0.0099 - val_acc: 0.9777 - val_loss: 0.1218\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 479us/step - acc: 0.9965 - loss: 0.0104 - val_acc: 0.9733 - val_loss: 0.1388\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 441us/step - acc: 0.9963 - loss: 0.0106 - val_acc: 0.9760 - val_loss: 0.1375\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 444us/step - acc: 0.9966 - loss: 0.0095 - val_acc: 0.9775 - val_loss: 0.1313\n"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(784,))\n",
    "x1 = layers.Dense(128, activation='relu')(input_tensor)\n",
    "x2 = layers.Dense(32, activation='relu')(x1)\n",
    "output_tensor = layers.Dense(10, activation='softmax')(x2)\n",
    "mlpclf3 = models.Model(input_tensor, output_tensor)\n",
    "mlpclf3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "mlpclf3.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035aa8e6-ed2f-4157-b2fe-67e51152fdd7",
   "metadata": {},
   "source": [
    "So far, this one seems to be the best choice among the MLP networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
